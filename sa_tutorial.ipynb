{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sg4bOmBSf3f"
      },
      "source": [
        "# Sentiment Classification Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXSHqxlPSf3g"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p37ud1HBSf3g"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVc3LDwDSf3h",
        "outputId": "0026c9d7-6ea2-4ef6-e2e6-614d5c1993bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2500000 tweets loaded\n"
          ]
        }
      ],
      "source": [
        "tweets = []\n",
        "labels = []\n",
        "\n",
        "def load_tweets(filename, label):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tweets.append(line.rstrip())\n",
        "            labels.append(label)\n",
        "\n",
        "load_tweets('twitter-datasets/train_neg_full.txt', 0)\n",
        "load_tweets('twitter-datasets/train_pos_full.txt', 1)\n",
        "\n",
        "# Convert to NumPy array to facilitate indexing\n",
        "tweets = np.array(tweets)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f'{len(tweets)} tweets loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhXhis4KSf3h"
      },
      "source": [
        "# Build validation set\n",
        "We use 90% of tweets for training, and 10% for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6GUYV5tSf3h",
        "outputId": "1aa1ce7f-0f0d-43c6-9e36-b31640fc97de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2250000, 250000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(1) # Reproducibility!\n",
        "\n",
        "shuffled_indices = np.random.permutation(len(tweets))\n",
        "split_idx = int(0.9 * len(tweets))\n",
        "train_indices = shuffled_indices[:split_idx]\n",
        "val_indices = shuffled_indices[split_idx:]\n",
        "\n",
        "len(train_indices), len(val_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk8uVL1TSf3h"
      },
      "source": [
        "# Bag-of-words baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkQfi9ztSf3h"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# We only keep the 5000 most frequent words, both to reduce the computational cost and reduce overfitting\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "\n",
        "# Important: we call fit_transform on the training set, and only transform on the validation set\n",
        "X_train = vectorizer.fit_transform(tweets[train_indices])\n",
        "X_val = vectorizer.transform(tweets[val_indices])\n",
        "\n",
        "Y_train = labels[train_indices]\n",
        "Y_val = labels[val_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJTzlFfMSf3i"
      },
      "source": [
        "Now we train a logistic classifier..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GWOzciYSf3i",
        "outputId": "787a3a4c-7d02-41ac-8f76-4a27e1e837e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(C=1e5, max_iter=100)\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPT1bTEcSf3i"
      },
      "outputs": [],
      "source": [
        "Y_train_pred = model.predict(X_train)\n",
        "Y_val_pred = model.predict(X_val)\n",
        "\n",
        "train_accuracy = (Y_train_pred == Y_train).mean()\n",
        "val_accuracy = (Y_val_pred == Y_val).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFWTuja9Sf3i",
        "outputId": "c55c6697-0733-46f8-a7df-f974461c4525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (training set): 0.80527\n",
            "Accuracy (validation set): 0.80324\n"
          ]
        }
      ],
      "source": [
        "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
        "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3mPsKVaSf3i"
      },
      "source": [
        "# Model interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV4lhyeYSf3i",
        "outputId": "dc65b262-2ab7-441b-d818-b01238401ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- Top 10 negative words\n",
            "paperback -7.733715089476916\n",
            "hardcover -6.749721263599857\n",
            "sadtweet -4.022199848355659\n",
            "audio -3.8849876465208113\n",
            "misc -3.7553966613158702\n",
            "depressing -3.63732789050843\n",
            "gutted -3.5956754364460863\n",
            "wahhh -3.521614632401248\n",
            "apparel -3.217069805985382\n",
            "fml -3.1400132802859333\n",
            "\n",
            "---- Top 10 positive words\n",
            "thx 2.057920021771283\n",
            "cantsayno 2.059860424345465\n",
            "blessed 2.1638415390167096\n",
            "smiling 2.195291992774262\n",
            "worries 2.3181506563261367\n",
            "ifindthatattractive 2.4271197000353912\n",
            "harrypotterchatuplines 2.4633027181285185\n",
            "smartnokialumia 3.1312606562595673\n",
            "waystomakemehappy 3.382280627938651\n",
            "yougetmajorpointsif 4.349066000550539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_features = model.coef_[0]\n",
        "sorted_features = np.argsort(model_features)\n",
        "top_neg = sorted_features[:10]\n",
        "top_pos = sorted_features[-10:]\n",
        "\n",
        "mapping = vectorizer.get_feature_names()\n",
        "\n",
        "print('---- Top 10 negative words')\n",
        "for i in top_neg:\n",
        "    print(mapping[i], model_features[i])\n",
        "print()\n",
        "\n",
        "print('---- Top 10 positive words')\n",
        "for i in top_pos:\n",
        "    print(mapping[i], model_features[i])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EvCrnM6Sf3j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}