{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "85A6c5lbpNP1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TzoWLRS6pJoJ"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pgNkK2yupLSe"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    tweets = []\n",
    "    labels = []\n",
    "\n",
    "    def load_tweets(filename, label):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tweets.append(line.rstrip())\n",
    "                labels.append(label)\n",
    "\n",
    "    load_tweets('twitter-datasets/train_neg_full.txt', 0)\n",
    "    load_tweets('twitter-datasets/train_pos_full.txt', 1)\n",
    "\n",
    "    # Convert to NumPy array to facilitate indexing\n",
    "    tweets = np.array(tweets)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f'{len(tweets)} tweets loaded')\n",
    "\n",
    "    return tweets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_XwLn88tpf5g"
   },
   "outputs": [],
   "source": [
    "def split_dataset(tweets):\n",
    "    np.random.seed(1) # Reproducibility!\n",
    "\n",
    "    shuffled_indices = np.random.permutation(len(tweets))\n",
    "    split_idx = int(0.9 * len(tweets))\n",
    "    train_indices = shuffled_indices[:split_idx]\n",
    "    val_indices = shuffled_indices[split_idx:]\n",
    "\n",
    "    print(\"Size train set:\", len(train_indices))\n",
    "    print(\"Size validation set:\", len(val_indices))\n",
    "\n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxuewT7E4-Uv",
    "outputId": "acee1d23-5e37-4dc3-8627-b4b63abb8cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500000 tweets loaded\n",
      "Size train set: 2250000\n",
      "Size validation set: 250000\n"
     ]
    }
   ],
   "source": [
    "tweets, labels = load_dataset()\n",
    "train_indices, val_indices = split_dataset(tweets)\n",
    "Y_train, Y_val = labels[train_indices], labels[val_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z3HKQYoTt9UA"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nEBbyztypW5a"
   },
   "outputs": [],
   "source": [
    "class Models:\n",
    "    def __init__(self, embedding, dimension=100):\n",
    "        self.dimension = dimension\n",
    "\n",
    "        self.embedding_index = embedding\n",
    "\n",
    "        print('Loading vectors with dimension', dimension)\n",
    "        print('Found %s word vectors' % len(self.embedding_index))\n",
    "\n",
    "    def create_tweet_embeddings(self, tweets, train_idx, val_idx):\n",
    "        X = np.zeros((len(tweets), self.dimension))\n",
    "\n",
    "        for i, tw in enumerate(tweets):\n",
    "            word_count = 0\n",
    "            tw = tw.split()\n",
    "\n",
    "            for word in tw:\n",
    "                if self.embedding_index.has_index_for(word):\n",
    "                    X[i] += self.embedding_index.get_vector(word)\n",
    "                    word_count += 1\n",
    "\n",
    "            if word_count: X[i] = X[i] / word_count\n",
    "\n",
    "        print(\"Tweet embeddings created\")\n",
    "\n",
    "        return X[train_idx], X[val_idx]\n",
    "\n",
    "    def logistic_regression(self, X_train, Y_train, X_val, Y_val):\n",
    "        model = LogisticRegression(random_state=0, C=1e5, max_iter=300)\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        print(\"Model trained\")\n",
    "\n",
    "        Y_train_pred = model.predict(X_train)\n",
    "        Y_val_pred = model.predict(X_val)\n",
    "\n",
    "        train_accuracy = (Y_train_pred == Y_train).mean()\n",
    "        val_accuracy = (Y_val_pred == Y_val).mean()\n",
    "\n",
    "        return train_accuracy, val_accuracy\n",
    "\n",
    "    def mlp(self, X_train, Y_train, X_val, Y_val):\n",
    "        model = MLPClassifier(random_state=0, alpha = 0.7, max_iter=400)\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        print(\"Model trained\")\n",
    "\n",
    "        Y_train_pred = model.predict(X_train)\n",
    "        Y_val_pred = model.predict(X_val)\n",
    "\n",
    "        train_accuracy = (Y_train_pred == Y_train).mean()\n",
    "        val_accuracy = (Y_val_pred == Y_val).mean()\n",
    "\n",
    "        return train_accuracy, val_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "s_S2Jjg9pV2q"
   },
   "source": [
    "# Glove 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ycpEwk3VjC0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wg6Z2t4G31yO",
    "outputId": "7ccd13a2-2ddf-4cf4-db99-eae7a026ed71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors with dimension 100\n",
      "Found 1193514 word vectors\n"
     ]
    }
   ],
   "source": [
    "glove_model = Models(embedding=glove_embedding, dimension=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uo7BpMRc5W8V",
    "outputId": "d8b4cedb-9343-436d-ee87-ae6b190050ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet embeddings created\n",
      "Shape X_train: (2250000, 100)\n",
      "Shape X_val: (250000, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = glove_model.create_tweet_embeddings(tweets, train_indices, val_indices)\n",
    "print('Shape X_train:', X_train.shape)\n",
    "print('Shape X_val:', X_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9UF-hivgqHnP"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txrfIApLqJvv",
    "outputId": "5032459a-0134-48d8-d89b-c372aa32ffce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Accuracy (training set): 0.76393\n",
      "Accuracy (validation set): 0.76321\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, val_accuracy = glove_model.logistic_regression(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TNpYmCZ9qLB-"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrVTrVMM6-Gg",
    "outputId": "33d05f8f-b3f4-4958-dce3-80500a09f863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Accuracy (training set): 0.77693\n",
      "Accuracy (validation set): 0.77805\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, val_accuracy = glove_model.mlp(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "s_S2Jjg9pV2q"
   },
   "source": [
    "# Glove 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ycpEwk3VjC0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.5% 754.6/758.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wg6Z2t4G31yO",
    "outputId": "7ccd13a2-2ddf-4cf4-db99-eae7a026ed71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors with dimension 200\n",
      "Found 1193514 word vectors\n"
     ]
    }
   ],
   "source": [
    "glove_model = Models(embedding=glove_embedding, dimension=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uo7BpMRc5W8V",
    "outputId": "d8b4cedb-9343-436d-ee87-ae6b190050ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet embeddings created\n",
      "Shape X_train: (2250000, 200)\n",
      "Shape X_val: (250000, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = glove_model.create_tweet_embeddings(tweets, train_indices, val_indices)\n",
    "print('Shape X_train:', X_train.shape)\n",
    "print('Shape X_val:', X_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9UF-hivgqHnP"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txrfIApLqJvv",
    "outputId": "5032459a-0134-48d8-d89b-c372aa32ffce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Accuracy (training set): 0.78214\n",
      "Accuracy (validation set): 0.78108\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, val_accuracy = glove_model.logistic_regression(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TNpYmCZ9qLB-"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrVTrVMM6-Gg",
    "outputId": "33d05f8f-b3f4-4958-dce3-80500a09f863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Accuracy (training set): 0.79331\n",
      "Accuracy (validation set): 0.79408\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, val_accuracy = glove_model.mlp(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iRgy3TnPqMRQ"
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "on0m1s37nJi4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word2vec_embedding = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kejJvFIy6t6O",
    "outputId": "c65b07e1-14e7-44cd-9d61-87506978f09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors with dimension 300\n",
      "Found 3000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Models(embedding=word2vec_embedding, dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fG3R23Nd61j1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet embeddings created\n",
      "Shape X_train: (2250000, 300)\n",
      "Shape X_val: (250000, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = word2vec_model.create_tweet_embeddings(tweets, train_indices, val_indices)\n",
    "print('Shape X_train:', X_train.shape)\n",
    "print('Shape X_val:', X_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "D1j9iMMQqPUE"
   },
   "source": [
    "## Logisic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GQ7CZvm4qNSQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Accuracy (training set): 0.74975\n",
      "Accuracy (validation set): 0.74992\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, val_accuracy = word2vec_model.logistic_regression(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "o1Ui1ZNRqRf5"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1RMBoyDxqSdS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Accuracy (training set): 0.74993\n",
      "Accuracy (validation set): 0.74994\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, val_accuracy = word2vec_model.mlp(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TzoWLRS6pJoJ",
    "D1j9iMMQqPUE",
    "o1Ui1ZNRqRf5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
